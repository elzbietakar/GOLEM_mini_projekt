{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "typ pustego\n",
      "<class 'numpy.uint8'>\n",
      "(10000, 3072)\n",
      "typ pustego\n",
      "<class 'numpy.uint8'>\n",
      "(10000, 3072)\n",
      "typ pustego\n",
      "<class 'numpy.uint8'>\n",
      "(10000, 3072)\n",
      "typ pustego\n",
      "<class 'numpy.uint8'>\n",
      "(10000, 3072)\n",
      "typ pustego\n",
      "<class 'numpy.uint8'>\n",
      "<class 'numpy.uint8'>\n",
      "[[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ...\n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ...\n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ...\n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ...\n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ...\n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ...\n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n",
      "b'batch_label'   <class 'bytes'>\n",
      "b'labels'   <class 'list'>\n",
      "b'data'   <class 'numpy.ndarray'>\n",
      "b'filenames'   <class 'list'>\n",
      "tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
      "         [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
      "         [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
      "         ...,\n",
      "         [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
      "         [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
      "         [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
      "\n",
      "        [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
      "         [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
      "         [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
      "         ...,\n",
      "         [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
      "         [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
      "         [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
      "\n",
      "        [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
      "         [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
      "         [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
      "         ...,\n",
      "         [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
      "         [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
      "         [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from class_cifar_dataset import CifarDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def reshape(data):\n",
    "    data = data.reshape(len(data),3, 32, 32)\n",
    "    data = data.transpose(0, 2, 3, 1)\n",
    "    return data\n",
    "\n",
    "train_labels = []\n",
    "train_data = np.empty((0, 3072), dtype=np.uint8)\n",
    "test_labels = []\n",
    "test_data = np.empty((0, 3072), dtype=np.uint8)\n",
    "\n",
    "\n",
    "for i in range(1, 6):\n",
    "    file = f'cifar-10-batches-py/data_batch_{i}'\n",
    "    dictionary = unpickle(file)\n",
    "    print(dictionary[b'data'].shape)\n",
    "    train_labels += dictionary[b'labels']\n",
    "    train_data = np.concatenate([train_data, dictionary[b'data']])\n",
    "    print(\"typ pustego\")\n",
    "    print(type(train_data[0][0]))\n",
    "    # print(type(train_data))\n",
    "    #print(type( dictionary[b'data']))\n",
    "\n",
    "\n",
    "file = f'cifar-10-batches-py/test_batch'\n",
    "dictionary = unpickle(file)\n",
    "test_labels = dictionary[b'labels']\n",
    "test_data = dictionary[b'data']\n",
    "\n",
    "train_data = train_data.reshape(len(train_data), 3, 32, 32)\n",
    "train_data = train_data.transpose(0, 2, 3, 1)\n",
    "test_data = test_data.reshape(len(test_data), 3, 32, 32)\n",
    "test_data = test_data.transpose(0, 2, 3, 1)\n",
    "\n",
    "print(type(train_data[0][0][0][0]))\n",
    "print(train_data[0])\n",
    "\n",
    "\n",
    "\n",
    "for key in dictionary.keys():\n",
    "    print(f'{key}   {type(dictionary[key])}')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = CifarDataset(train_labels, train_data, transform)\n",
    "train_dataloader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "testset = CifarDataset(test_labels, test_data, transform)\n",
    "test_dataloader = DataLoader(testset, batch_size=4, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# print(\"typ\")\n",
    "\n",
    "print(trainset.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.702\n",
      "[1,  4000] loss: 0.739\n",
      "[1,  6000] loss: 0.747\n",
      "[1,  8000] loss: 0.753\n",
      "[1, 10000] loss: 0.786\n",
      "[1, 12000] loss: 0.779\n",
      "[2,  2000] loss: 0.664\n",
      "[2,  4000] loss: 0.708\n",
      "[2,  6000] loss: 0.767\n",
      "[2,  8000] loss: 0.750\n",
      "[2, 10000] loss: 0.767\n",
      "[2, 12000] loss: 0.766\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  car   frog  dog   car  \n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 61 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
